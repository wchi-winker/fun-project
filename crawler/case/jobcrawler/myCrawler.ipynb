{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757d4320",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'case'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22613/550851811.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobcrawler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# from case.jobcrawler import webrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from case.jobcrawler import Storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# from case.jobcrawler import storagebase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'case'"
     ]
    }
   ],
   "source": [
    "from case.jobcrawler import webrequests\n",
    "# from case.jobcrawler import webrequests\n",
    "# from case.jobcrawler import Storage\n",
    "# from case.jobcrawler import storagebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4a18c1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'case.jobcrawler' has no attribute 'jobobj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11710/1942596811.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobcrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'case.jobcrawler' has no attribute 'jobobj'"
     ]
    }
   ],
   "source": [
    "import case\n",
    "job = case.jobcrawler.jobobj.Job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77b67ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import case.demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b263c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始爬取北京岗位为elt的第1页数据\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'webrequests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11710/1096262972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mjobCrawler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJobCrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mjobCrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_11710/1096262972.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'https://www.jobui.com/jobs?jobKw={job}&cityKw={city}&n={page}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"开始爬取{city}岗位为{job}的第{page}页数据\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_bs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mpage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11710/1096262972.py\u001b[0m in \u001b[0;36mparser_bs\u001b[0;34m(self, url, city)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparser_bs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# # 【2】获取URL文本内容\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mjob_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'c-job-list'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'webrequests' is not defined"
     ]
    }
   ],
   "source": [
    "# %load jobcrawler.py\n",
    "'''\n",
    "需求：\n",
    "针对职友集岗位数据爬取\n",
    "1、城市数据通过列表修改\n",
    "2、岗位数据通过列表修改\n",
    "3、爬取1、2中所有城市对应所有岗位数据，存储到jobs.txt\n",
    "'''\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from case.jobcrawler import webrequests\n",
    "from case.jobcrawler import jobobj\n",
    "from case.jobcrawler import storagebase\n",
    "\n",
    "\n",
    "# 岗位数据爬取类\n",
    "class JobCrawler:\n",
    "\n",
    "    # 加载配置文件\n",
    "    def __load_conf(self):\n",
    "        citys = []\n",
    "        jobs = []\n",
    "        with open('./config/city.conf', encoding='utf-8') as f:\n",
    "            for city in f:\n",
    "                citys.append(city.strip())\n",
    "\n",
    "        with open('./config/jobname.conf', encoding='utf-8') as f:\n",
    "            for job in f:\n",
    "                jobs.append(job.strip())\n",
    "\n",
    "        return citys, jobs\n",
    "\n",
    "    # 通过城市、岗位、页数定位网页\n",
    "    def start(self):\n",
    "\n",
    "        citys, jobs = self.__load_conf()\n",
    "\n",
    "        for city in citys:\n",
    "            for job in jobs:\n",
    "                page = 1\n",
    "                while True:\n",
    "                    url = f'https://www.jobui.com/jobs?jobKw={job}&cityKw={city}&n={page}'\n",
    "                    print(f\"开始爬取{city}岗位为{job}的第{page}页数据\")\n",
    "                    flag = self.parser_bs(url, city)\n",
    "                    if flag:\n",
    "                        page += 1\n",
    "                        time.sleep(random.randint(1, 2))\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "    # 针对岗位基本信息进行爬取及解析\n",
    "    def parser_bs(self, url, city):\n",
    "        # # 【2】获取URL文本内容\n",
    "        data = webrequests.get_data(url)\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        job_lists = soup.find_all('div', attrs={'class': 'c-job-list'})\n",
    "        if job_lists:\n",
    "            # 获取基本信息\n",
    "            self.parser_info(job_lists, city)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # 针对网页数据进行解析\n",
    "    def parser_info(self, job_lists, city):\n",
    "        for item in job_lists:\n",
    "            job = jobobj.Job()\n",
    "            job.jobname = item.find('h3').text\n",
    "            spans = item.find('div', attrs={'class': 'job-desc'}).find_all('span')\n",
    "            job.exp = spans[0].text\n",
    "            job.degree = spans[1].text\n",
    "            job.salary = spans[2].text\n",
    "            job.company = item.find('a', attrs={'class': 'job-company-name'}).text\n",
    "            # 点击量信息 coding...\n",
    "            job.hit = item.find('span', attrs={'class': 'job-desc'}).text.strip()\n",
    "\n",
    "            # 岗位详情地址\n",
    "            jobaddr = item.find('a', attrs={'class': 'job-name'}).get('href')\n",
    "            joburl = f'https://www.jobui.com/{jobaddr}'\n",
    "\n",
    "            # time.sleep(random.randint(1, 5))\n",
    "            # 获取详情信息\n",
    "            self.parser_detail(joburl, job)\n",
    "            # 数据库存储\n",
    "            storage = storagebase.JobStorage()\n",
    "            storage.insert(job)\n",
    "\n",
    "    # 针对岗位详情页进行数据获取及解析\n",
    "    def parser_detail(self, url, job):\n",
    "\n",
    "        try:\n",
    "            data = webrequests.get_data(url)\n",
    "            soup = BeautifulSoup(data, 'html.parser')\n",
    "            # 更新时间\n",
    "            job.updatetime = soup.find('span', attrs={'class': 'fs16 gray9'}).text.strip()\n",
    "            # 岗位详情、地址、来源网站  coding.... ------待完成\n",
    "            \n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jobCrawler = JobCrawler()\n",
    "    jobCrawler.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1657d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class:\n",
    "\n",
    "    def __init__(self):\n",
    "        # 定义Job类中的成员属性\n",
    "        self.jobname = ''\n",
    "        self.exp = ''\n",
    "        self.degree = ''\n",
    "        self.salary = 10000\n",
    "        self.company = ''\n",
    "        self.hit = 1000\n",
    "        self.industry = ''\n",
    "        self.scale =''\n",
    "        self.city = '北京'\n",
    "\n",
    "        #岗位职责\n",
    "        self.duty = ''\n",
    "        #更新时间\n",
    "        self.updatetime = ''\n",
    "        #位置信息\n",
    "        self.location = ''\n",
    "        #数据获取时间\n",
    "        self.collecttime = ''\n",
    "        #网站来源\n",
    "        self.website= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2190ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'text'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 获取网页数据文本信息\n",
    "def get_data(url, encoding='utf8'):\n",
    "    headers = {'User-Agent': get_headers()}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.encoding = encoding\n",
    "    data = response.text\n",
    "    return data\n",
    "\n",
    "\n",
    "# 获取headers\n",
    "def get_headers():\n",
    "    return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "\n",
    "\n",
    "def parser_detail(url, job):\n",
    "\n",
    "        try:\n",
    "            data = get_data(url)\n",
    "            soup = BeautifulSoup(data, 'html.parser')\n",
    "            # 更新时间\n",
    "            job.updatetime = soup.find('span', attrs={'class': 'fs16 gray9'}).text.strip()\n",
    "            # 岗位详情、地址、来源网站  coding.... ------待完成\n",
    "            job.jobname = soup.find('span', attrs={'class': 'pos_name'}).text.strip()\n",
    "            job.exp = soup.find('span', attrs={'class': 'pos_require'}).text.strip()\n",
    "            job.degree = soup.find('span', attrs={'class': 'pos_require'}).text.strip()\n",
    "            job.salary = soup.find('span', attrs={'class': 'pos_salary'}).text.strip()\n",
    "            job.company = soup.find('span', attrs={'class': 'pos_info'}).text.strip()\n",
    "            job.website= soup.find('span', attrs={'class': 'report'}).text.strip()\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "url = 'http://www.jobcn.com/position/detail.xhtml?redirect=0&posId=4202327&comId=465792&s=search/advanced&acType=1'\n",
    "job = Job()\n",
    "parser_detail(url, job)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
